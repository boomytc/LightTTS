import sys
sys.path.append('Matcha-TTS')
import os
import argparse
import gradio as gr
import numpy as np
import torch
import torchaudio
import random
import librosa
import logging
from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
from cosyvoice.utils.common import set_all_random_seed

# ç¦ç”¨Gradioçš„è°ƒè¯•æ—¥å¿—
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"

# å¯ç”¨PEFTåŽç«¯
os.environ["DIFFUSERS_PEFT_BACKEND"] = "TRUE"

# ç¦ç”¨DEBUGçº§åˆ«çš„æ—¥å¿—è¾“å‡º
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("uvicorn").setLevel(logging.WARNING)
logging.getLogger("gradio").setLevel(logging.WARNING)

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

inference_mode_list = ['é›¶æ ·æœ¬è¯­éŸ³å…‹éš†', 'è·¨è¯­è¨€è¯­éŸ³åˆæˆ', 'ç²¾ç»†æŽ§åˆ¶åˆæˆ', 'æŒ‡ä»¤æŽ§åˆ¶åˆæˆ']
stream_mode_list = [('å¦', False), ('æ˜¯', True)]
max_val = 0.8

mode_descriptions = {
    'é›¶æ ·æœ¬è¯­éŸ³å…‹éš†': '1. è¾“å…¥åˆæˆæ–‡æœ¬ã€‚\n2. ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘ (å»ºè®®5-15ç§’)ã€‚\n3. è¾“å…¥Promptæ–‡æœ¬ (å¯¹å‚è€ƒéŸ³é¢‘å†…å®¹çš„æè¿°)ã€‚\n4. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®ã€‚',
    'è·¨è¯­è¨€è¯­éŸ³åˆæˆ': '1. è¾“å…¥åˆæˆæ–‡æœ¬ (å¯ä¸Žå‚è€ƒéŸ³é¢‘è¯­è¨€ä¸åŒ)ã€‚\n2. ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘ (å»ºè®®5-15ç§’)ã€‚\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®ã€‚',
    'ç²¾ç»†æŽ§åˆ¶åˆæˆ': '1. è¾“å…¥åˆæˆæ–‡æœ¬ã€‚\n2. ä¸Šä¼ æˆ–å½•åˆ¶é«˜è´¨é‡ã€å¯Œæœ‰è¡¨çŽ°åŠ›çš„å‚è€ƒéŸ³é¢‘ (å»ºè®®5-15ç§’)ã€‚\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®ã€‚\n(æ³¨æ„ï¼šæ­¤æ¨¡å¼å½“å‰å®žçŽ°ä¸Žâ€œè·¨è¯­è¨€è¯­éŸ³åˆæˆâ€ç›¸ä¼¼)',
    'æŒ‡ä»¤æŽ§åˆ¶åˆæˆ': '1. è¾“å…¥åˆæˆæ–‡æœ¬ã€‚\n2. ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘ (å»ºè®®5-15ç§’)ã€‚\n3. è¾“å…¥æŒ‡ä»¤æ–‡æœ¬ (ä¾‹å¦‚ï¼šç”¨å››å·è¯è¯´è¿™å¥è¯)ã€‚\n4. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®ã€‚'
}

def generate_seed():
    seed = random.randint(1, 100000000)
    return {
        "__type__": "update",
        "value": seed
    }

def postprocess(speech, top_db=60, hop_length=220, win_length=440):
    speech, _ = librosa.effects.trim(
        speech, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    if speech.abs().max() > max_val:
        speech = speech / speech.abs().max() * max_val
    speech = torch.concat([speech, torch.zeros(1, int(cosyvoice.sample_rate * 0.2))], dim=1)
    return speech

def generate_audio(tts_text, mode_checkbox_group, prompt_text, prompt_wav_upload, prompt_wav_record, instruct_text,
                   seed, stream, speed):
    prompt_wav = prompt_wav_upload or prompt_wav_record
    
    if prompt_wav is None:
        yield (cosyvoice.sample_rate, default_data)
        return
    
    if torchaudio.info(prompt_wav).sample_rate < prompt_sr:
        yield (cosyvoice.sample_rate, default_data)
        return
    
    prompt_speech_16k = postprocess(load_wav(prompt_wav, prompt_sr))
    set_all_random_seed(seed)
    
    if mode_checkbox_group == 'é›¶æ ·æœ¬è¯­éŸ³å…‹éš†':
        if not prompt_text:
            yield (cosyvoice.sample_rate, default_data)
            return
        for i in cosyvoice.inference_zero_shot(tts_text, prompt_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())
    
    elif mode_checkbox_group == 'è·¨è¯­è¨€è¯­éŸ³åˆæˆ':
        for i in cosyvoice.inference_cross_lingual(tts_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())
    
    elif mode_checkbox_group == 'ç²¾ç»†æŽ§åˆ¶åˆæˆ':
        for i in cosyvoice.inference_cross_lingual(tts_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())
    
    elif mode_checkbox_group == 'æŒ‡ä»¤æŽ§åˆ¶åˆæˆ':
        if not instruct_text:
            yield (cosyvoice.sample_rate, default_data)
            return
        for i in cosyvoice.inference_instruct2(tts_text, instruct_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())

def main():
    with gr.Blocks() as demo:
        gr.Markdown("### LightTTS è¯­éŸ³åˆæˆç³»ç»Ÿ")
        
        tts_text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=2, value="æ”¶åˆ°å¥½å‹ä»Žè¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸Žæ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚")
        
        with gr.Row():
            mode_checkbox_group = gr.Radio(choices=inference_mode_list, label='æŽ¨ç†æ¨¡å¼', value=inference_mode_list[0])
            stream = gr.Radio(choices=stream_mode_list, label='æµå¼æŽ¨ç†', value=stream_mode_list[0][1])
            speed = gr.Number(value=1, label="é€Ÿåº¦", minimum=0.5, maximum=2.0, step=0.1)
            with gr.Column():
                seed_button = gr.Button(value="ðŸŽ²")
                seed = gr.Number(value=0, label="ç§å­")

        mode_description_display = gr.Markdown(value=mode_descriptions[inference_mode_list[0]])

        with gr.Row():
            prompt_wav_upload = gr.Audio(sources='upload', type='filepath', label='éŸ³é¢‘æ–‡ä»¶')
            prompt_wav_record = gr.Audio(sources='microphone', type='filepath', label='å½•åˆ¶éŸ³é¢‘')
        
        prompt_text = gr.Textbox(label="promptæ–‡æœ¬", value='å¸Œæœ›ä½ ä»¥åŽèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚')
        instruct_text = gr.Textbox(label="æŒ‡ä»¤æ–‡æœ¬", value='ç”¨å››å·è¯è¯´è¿™å¥è¯')
        
        generate_button = gr.Button("ç”ŸæˆéŸ³é¢‘")
        audio_output = gr.Audio(label="åˆæˆéŸ³é¢‘", autoplay=True, streaming=True)

        seed_button.click(generate_seed, inputs=[], outputs=seed)
        generate_button.click(generate_audio,
                              inputs=[tts_text, mode_checkbox_group, prompt_text, prompt_wav_upload, prompt_wav_record, instruct_text,
                                      seed, stream, speed],
                              outputs=[audio_output])

        def update_description(mode):
            return mode_descriptions.get(mode, "è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å¼ä»¥æŸ¥çœ‹è¯´æ˜Žã€‚")

        mode_checkbox_group.change(update_description, inputs=mode_checkbox_group, outputs=mode_description_display)
    
    demo.queue(max_size=18, default_concurrency_limit=6)
    demo.launch(inbrowser=True, server_name='127.0.0.1', server_port=args.port, share=False)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--port',
                        type=int,
                        default=8008)
    parser.add_argument('--model_dir',
                        type=str,
                        default='models/CosyVoice2-0.5B',
                        help='local path or modelscope repo id')
    args = parser.parse_args()
    
    cosyvoice = CosyVoice2(args.model_dir)

    prompt_sr = 16000
    default_data = np.zeros(cosyvoice.sample_rate)
    main()